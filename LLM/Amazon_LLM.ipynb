{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Leveraging Large Language Models (LLMs) to enhance product descriptions and SEO\n",
    "\n",
    "- Develop a system using an LLM to generate optimized product descriptions.\n",
    "- Analyze the impact of AI-generated content on search rankings and conversion rates.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandon/Library/Python/3.9/lib/python/site-packages/transformers/tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer, Trainer, TrainingArguments\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "from datasets import load_dataset, DatasetDict\n",
    "\n",
    "amazon_dataset = load_dataset(\"csv\", data_files='./amazon.csv')\n",
    "# Load pre-trained model and tokenizer\n",
    "model_name = \"distilgpt2\"\n",
    "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 1465/1465 [00:00<00:00, 19756.72 examples/s]\n"
     ]
    }
   ],
   "source": [
    "def combine(row):\n",
    "    return {\n",
    "        'text': [f\"{product_id} {product_name} {category} {discounted_price} {actual_price} {discount_percentage} {rating} {rating_count} {about_product}\" for product_id, product_name, category, discounted_price, actual_price, discount_percentage, rating, rating_count, about_product in zip(row['product_id'], row['product_name'], row['category'], row['discounted_price'], row['actual_price'], row['discount_percentage'], row['rating'], row[\"rating_count\"], row['about_product'])]\n",
    "    }\n",
    "\n",
    "\n",
    "amazon_dataset = amazon_dataset.map(combine, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 1465/1465 [00:01<00:00, 1150.08 examples/s]\n"
     ]
    }
   ],
   "source": [
    "def tokenize_function(row):\n",
    "    tokenized_inputs = tokenizer(\n",
    "        row['text'], truncation=True, padding='max_length', max_length=524)\n",
    "    tokenized_inputs['labels'] = tokenized_inputs['input_ids'].copy()\n",
    "    return tokenized_inputs\n",
    "\n",
    "\n",
    "tokenized_dataset = amazon_dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "train_test_split = tokenized_dataset['train'].train_test_split(test_size=0.1)\n",
    "\n",
    "tokenized_dataset = DatasetDict({\n",
    "    'train': train_test_split['train'],\n",
    "    'validation': train_test_split['test']\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['product_id', 'product_name', 'category', 'discounted_price', 'actual_price', 'discount_percentage', 'rating', 'rating_count', 'about_product', 'user_id', 'user_name', 'review_id', 'review_title', 'review_content', 'img_link', 'product_link', 'text', 'input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 1318\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['product_id', 'product_name', 'category', 'discounted_price', 'actual_price', 'discount_percentage', 'rating', 'rating_count', 'about_product', 'user_id', 'user_name', 'review_id', 'review_title', 'review_content', 'img_link', 'product_link', 'text', 'input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 147\n",
       "    })\n",
       "})"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)\n",
      "Cell \u001b[0;32mIn[18], line 18\u001b[0m\n",
      "\u001b[1;32m      1\u001b[0m training_args \u001b[38;5;241m=\u001b[39m TrainingArguments(\n",
      "\u001b[1;32m      2\u001b[0m     output_dir \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./results\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n",
      "\u001b[1;32m      3\u001b[0m     eval_strategy \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mepoch\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n",
      "\u001b[0;32m   (...)\u001b[0m\n",
      "\u001b[1;32m      8\u001b[0m     weight_decay \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.01\u001b[39m,\n",
      "\u001b[1;32m      9\u001b[0m )\n",
      "\u001b[1;32m     11\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(\n",
      "\u001b[1;32m     12\u001b[0m     model \u001b[38;5;241m=\u001b[39m model,\n",
      "\u001b[1;32m     13\u001b[0m     args \u001b[38;5;241m=\u001b[39m training_args,\n",
      "\u001b[1;32m     14\u001b[0m     train_dataset \u001b[38;5;241m=\u001b[39m tokenized_dataset[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m],\n",
      "\u001b[1;32m     15\u001b[0m     eval_dataset\u001b[38;5;241m=\u001b[39mtokenized_dataset[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvalidation\u001b[39m\u001b[38;5;124m'\u001b[39m],\n",
      "\u001b[1;32m     16\u001b[0m )\n",
      "\u001b[0;32m---> 18\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;32m     19\u001b[0m trainer\u001b[38;5;241m.\u001b[39mevaluate()\n",
      "\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/transformers/trainer.py:2052\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n",
      "\u001b[1;32m   2050\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n",
      "\u001b[1;32m   2051\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;32m-> 2052\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n",
      "\u001b[1;32m   2053\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m   2054\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m   2055\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m   2056\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m   2057\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/transformers/trainer.py:2390\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n",
      "\u001b[1;32m   2387\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39maccumulate(model):\n",
      "\u001b[1;32m   2388\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining_step(model, inputs)\n",
      "\u001b[0;32m-> 2390\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n",
      "\u001b[1;32m   2391\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n",
      "\u001b[1;32m   2392\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n",
      "\u001b[1;32m   2393\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n",
      "\u001b[1;32m   2394\u001b[0m ):\n",
      "\u001b[1;32m   2395\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n",
      "\u001b[1;32m   2396\u001b[0m     tr_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n",
      "\u001b[1;32m   2397\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    learning_rate=5e-5,\n",
    "    per_device_train_batch_size=2,\n",
    "    per_device_eval_batch_size=2,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset['train'],\n",
    "    eval_dataset=tokenized_dataset['validation'],\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./fine_tuned_distilgpt2/tokenizer_config.json',\n",
       " './fine_tuned_distilgpt2/special_tokens_map.json',\n",
       " './fine_tuned_distilgpt2/vocab.json',\n",
       " './fine_tuned_distilgpt2/merges.txt',\n",
       " './fine_tuned_distilgpt2/added_tokens.json')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model.save_pretrained('./fine_tuned_distilgpt2')\n",
    "tokenizer.save_pretrained('./fine_tuned_distilgpt2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandon/Library/Python/3.9/lib/python/site-packages/transformers/generation/configuration_utils.py:601: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.7` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/Users/brandon/Library/Python/3.9/lib/python/site-packages/transformers/generation/configuration_utils.py:606: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.95` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is the cheapest phone that right now? It's a good phone for the price range, but it's not a perfect phone, it has a bit of a lag, and it doesn't support Voice Assistant. It has some issues with the phone itself, which is why it is not available in the US. But it does support voice assistant, so you can get it for your family. So, you have to go for it. You can buy it in India, for example. And it\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "import torch\n",
    "\n",
    "# Load fine-tuned model and tokenizer\n",
    "model = GPT2LMHeadModel.from_pretrained('./fine_tuned_distilgpt2')\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('./fine_tuned_distilgpt2')\n",
    "\n",
    "# Assign the EOS token as the padding token if not set\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Move model to the appropriate device (GPU or CPU)\n",
    "device = torch.device(\n",
    "    'cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "# Input text for generation\n",
    "input_text = \"What is the cheapest phone that right now?\"\n",
    "inputs = tokenizer(input_text, return_tensors='pt',\n",
    "                   padding=True, truncation=True)\n",
    "\n",
    "input_ids = inputs['input_ids'].to(device)\n",
    "attention_mask = inputs['attention_mask'].to(device)\n",
    "\n",
    "# Generate text with diversity controls and the attention mask\n",
    "generated_text_ids = model.generate(\n",
    "    input_ids,\n",
    "    attention_mask=attention_mask,  # Pass attention mask\n",
    "    max_length=100,\n",
    "    num_return_sequences=1,\n",
    "    no_repeat_ngram_size=2,          # Prevent repetition of 2-grams\n",
    "    top_k=50,                        # Use top-k sampling\n",
    "    top_p=0.95,                      # Use nucleus sampling\n",
    "    temperature=0.7,                 # Lower temperature for more focused sampling\n",
    "    pad_token_id=tokenizer.eos_token_id  # Use EOS token as the pad token\n",
    ")\n",
    "\n",
    "# Decode the generated text\n",
    "generated_text = tokenizer.decode(\n",
    "    generated_text_ids[0], skip_special_tokens=True)\n",
    "\n",
    "# Print the generated text\n",
    "print(generated_text)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
